{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2\n",
    "\n",
    "#### Machine Learning in Korea University\n",
    "#### COSE362, Fall 2018\n",
    "#### Due : 11/26 (TUE) 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this assignment, you will learn various classification methods with given datasets.\n",
    "* Implementation detail: Anaconda 5.3 with python 3.7\n",
    "* Use given dataset. Please do not change train / valid / test split.\n",
    "* Use numpy, scikit-learn, and matplotlib library\n",
    "* You don't have to use all imported packages below. (some are optional). <br>\n",
    "Also, you can import additional packages in \"(Option) Other Classifiers\" part. \n",
    "* <b>*DO NOT MODIFY OTHER PARTS OF CODES EXCEPT \"Your Code Here\"*</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Additional packages\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your own packages if you need(only in scikit-learn, numpy, pandas).\n",
    "# Your Code Here\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "> 1. Load \"train.csv\". It includes all samples' features and labels.\n",
    "> 2. Training four types of classifiers(logistic regression, decision tree, random forest, support vector machine) and <b>validate</b> it in your own way. <b>(You can't get full credit if you don't conduct validation)</b>\n",
    "> 3. Optionally, if you would train your own classifier(e.g. ensembling or gradient boosting), you can evaluate your own model on the development data. <br>\n",
    "> 4. <b>You should submit your predicted results on test data with the selected classifier in your own manner.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task & dataset description\n",
    "1. 6 Features (1~6)<br>\n",
    "Feature 2, 4, 6 : Real-valued<br>\n",
    "Feature 1, 3, 5 : Categorical <br>\n",
    "\n",
    "2. Samples <br>\n",
    ">In development set : 2,000 samples <br>\n",
    ">In test set : 1,500 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load development dataset\n",
    "Load your development dataset. You should read <b>\"train.csv\"</b>. This is a classification task, and you need to preprocess your data for training your model. <br>\n",
    "> You need to use <b>1-of-K coding scheme</b>, to convert categorical features to one-hot vector. <br>\n",
    "> For example, if there are 3 categorical values, you can convert these features as [1,0,0], [0,1,0], [0,0,1] by 1-of-K coding scheme. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Plot class imbalance of train dataset\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Print shape of dex_X, dev_y, test_X, test_y\n",
      "(1600, 6) (1600,) (400, 6) (400,)\n",
      "--------------------------------------------------\n",
      "Print shape of drop_dex_X, drop_dev_y\n",
      "(1589, 6) (1589,)\n",
      "--------------------------------------------------\n",
      "Print shape of dev_X_one_hot, test_X_one_hot, drop_dev_X_one_hot\n",
      "(1600, 23) (400, 23) (1589, 23)\n"
     ]
    }
   ],
   "source": [
    "# For training your model, you need to convert categorical features to one-hot encoding vectors.\n",
    "# Your Code Here\n",
    "\n",
    "## Import Dataset\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "\n",
    "## Explore Class Imbalance\n",
    "target_count = {}\n",
    "\n",
    "for target in train.target.unique():\n",
    "    target_count[target] = len(train[train['target'] == target])\n",
    "    \n",
    "target_count_sorted = sorted(target_count.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "print(\"-----\" * 10)\n",
    "print(\"Plot class imbalance of train dataset\")\n",
    "plt.plot([x for x in range(len(target_count_sorted))], [val[1] for val in target_count_sorted])\n",
    "plt.show()\n",
    "\n",
    "## Split dev & test\n",
    "dev, test = train_test_split(train, test_size = 0.2, random_state=42)\n",
    "\n",
    "## Split X and y on dev & test sets\n",
    "dev_X, dev_y = dev.drop(columns = ['target']), dev['target']\n",
    "test_X, test_y = test.drop(columns = ['target']), test['target']\n",
    "\n",
    "print(\"-----\" * 10)\n",
    "print(\"Print shape of dex_X, dev_y, test_X, test_y\")\n",
    "print(dev_X.shape, dev_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "## Drop imbalance classes\n",
    "drop_dev = dev.drop(\n",
    "    dev[dev.target == 17].index |\n",
    "    dev[dev.target == 14].index |\n",
    "    dev[dev.target == 8].index\n",
    ")\n",
    "\n",
    "drop_dev_X, drop_dev_y = drop_dev.drop(columns = ['target']), drop_dev['target']\n",
    "print(\"-----\" * 10)\n",
    "print(\"Print shape of drop_dex_X, drop_dev_y\")\n",
    "print(drop_dev_X.shape, drop_dev_y.shape)\n",
    "\n",
    "## One-hot encoding\n",
    "dev_X_one_hot = pd.get_dummies(dev_X)\n",
    "test_X_one_hot = pd.get_dummies(test_X)\n",
    "drop_dev_X_one_hot = pd.get_dummies(drop_dev_X)\n",
    "print(\"-----\" * 10)\n",
    "print(\"Print shape of dev_X_one_hot, test_X_one_hot, drop_dev_X_one_hot\")\n",
    "print(dev_X_one_hot.shape, test_X_one_hot.shape, drop_dev_X_one_hot.shape)\n",
    "\n",
    "## Scaling\n",
    "sc = StandardScaler()\n",
    "dev_X_one_hot_std = pd.DataFrame(sc.fit_transform(dev_X_one_hot), index=dev_X_one_hot.index, columns=dev_X_one_hot.columns)\n",
    "test_X_one_hot_std = pd.DataFrame(sc.transform(test_X_one_hot), index=test_X_one_hot.index, columns=test_X_one_hot.columns)\n",
    "drop_dev_X_one_hot_std = pd.DataFrame(sc.fit_transform(drop_dev_X_one_hot), index=drop_dev_X_one_hot.index, columns=drop_dev_X_one_hot.columns)\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Train and validate your <b>logistic regression classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2504207324379794, 0.25121421478883604, 0.3115142537133155, 0.2592118937650194] 0.26809027367628757\n",
      "[0.1962456033675077, 0.23153212712226065, 0.17337480551019407, 0.18613749422063858] 0.19682250755515024\n",
      "0.1760708009003629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Training your logistic regression classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "\n",
    "## Define LogisticRegression Model\n",
    "model = LogisticRegression()\n",
    "\n",
    "## Define cross validation using KFold method\n",
    "## cv = ShuffleSplit(n_splits=4, test_size=0.25, random_state=0)\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "## lists that hold error on cross validation\n",
    "train_error = []\n",
    "valid_error = []\n",
    "\n",
    "## Train & Valid Model using Cross Validation\n",
    "for train_index, valid_index in cv.split(dev_X_one_hot):\n",
    "    X_train, X_valid = dev_X_one_hot.iloc[train_index, :], dev_X_one_hot.iloc[valid_index, :]\n",
    "    y_train, y_valid = dev_y.iloc[train_index], dev_y.iloc[valid_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_error.append(f1_score(model.predict(X_train), y_train, average='macro'))\n",
    "    valid_error.append(f1_score(model.predict(X_valid), y_valid, average='macro'))        \n",
    "\n",
    "print(train_error, np.mean(train_error))\n",
    "print(valid_error, np.mean(valid_error))\n",
    "\n",
    "## Report Test Error\n",
    "model.fit(dev_X_one_hot, dev_y)\n",
    "print(f1_score(model.predict(test_X_one_hot), test_y, average='macro'))\n",
    "    \n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4409761316493278, 0.40380456029746364, 0.44416576657145596, 0.4885619411351836] 0.4443770999133578\n",
      "[0.23947402097888737, 0.29328991403856775, 0.25774774285551394, 0.23259641082535001] 0.25577702217457976\n",
      "0.3186220458575611\n"
     ]
    }
   ],
   "source": [
    "# Training your logistic regression classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "\n",
    "## Define LogisticRegression Model\n",
    "model = LogisticRegression(penalty='l2', C = 300)\n",
    "\n",
    "## Define cross validation using KFold method\n",
    "## cv = ShuffleSplit(n_splits=4, test_size=0.25, random_state=42)\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "## lists that hold error on cross validation\n",
    "train_error = []\n",
    "valid_error = []\n",
    "\n",
    "## Train & Valid Model using Cross Validation\n",
    "for train_index, valid_index in cv.split(dev_X_one_hot):\n",
    "    X_train, X_valid = dev_X_one_hot.iloc[train_index, :], dev_X_one_hot.iloc[valid_index, :]\n",
    "    y_train, y_valid = dev_y.iloc[train_index], dev_y.iloc[valid_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_error.append(f1_score(model.predict(X_train), y_train, average='macro'))\n",
    "    valid_error.append(f1_score(model.predict(X_valid), y_valid, average='macro'))        \n",
    "\n",
    "print(train_error, np.mean(train_error))\n",
    "print(valid_error, np.mean(valid_error))\n",
    "\n",
    "## Report Test Error\n",
    "model.fit(dev_X_one_hot, dev_y)\n",
    "print(f1_score(model.predict(test_X_one_hot), test_y, average='macro'))\n",
    "    \n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39674940759550553, 0.38449026518087565, 0.410730563822636, 0.38172170715035686] 0.39342298593734354\n",
      "[0.2280561336658594, 0.23612400291462077, 0.19519072707368745, 0.23683446972433125] 0.22405133334462474\n",
      "0.2716515459458847\n"
     ]
    }
   ],
   "source": [
    "# Training your logistic regression classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "\n",
    "## Define LogisticRegression Model\n",
    "model = LogisticRegression(penalty='l2', C = 10)\n",
    "\n",
    "## Define cross validation using KFold method\n",
    "## cv = ShuffleSplit(n_splits=4, test_size=0.25, random_state=42)\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "## lists that hold error on cross validation\n",
    "train_error = []\n",
    "valid_error = []\n",
    "\n",
    "## Train & Valid Model using Cross Validation\n",
    "for train_index, valid_index in cv.split(dev_X_one_hot_std):\n",
    "    X_train, X_valid = dev_X_one_hot_std.iloc[train_index, :], dev_X_one_hot_std.iloc[valid_index, :]\n",
    "    y_train, y_valid = dev_y.iloc[train_index], dev_y.iloc[valid_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_error.append(f1_score(model.predict(X_train), y_train, average='macro'))\n",
    "    valid_error.append(f1_score(model.predict(X_valid), y_valid, average='macro'))        \n",
    "\n",
    "print(train_error, np.mean(train_error))\n",
    "print(valid_error, np.mean(valid_error))\n",
    "\n",
    "## Report Test Error\n",
    "model.fit(dev_X_one_hot_std, dev_y)\n",
    "print(f1_score(model.predict(test_X_one_hot_std), test_y, average='macro'))\n",
    "    \n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Train and validate your <b>decision tree classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0] 1.0\n",
      "[0.3050983575293927, 0.255394515374754, 0.284189297687113, 0.33619207751423574] 0.29521856202637387\n",
      "0.3402654010470154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Training your decision tree classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "\n",
    "## Define Decision Tree Model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "## Define cross validation using KFold method\n",
    "## cv = ShuffleSplit(n_splits=4, test_size=0.25, random_state=42)\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "## lists that hold error on cross validation\n",
    "train_error = []\n",
    "valid_error = []\n",
    "\n",
    "## Train & Valid Model using Cross Validation\n",
    "for train_index, valid_index in cv.split(dev_X_one_hot_std):\n",
    "    X_train, X_valid = dev_X_one_hot_std.iloc[train_index, :], dev_X_one_hot_std.iloc[valid_index, :]\n",
    "    y_train, y_valid = dev_y.iloc[train_index], dev_y.iloc[valid_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_error.append(f1_score(model.predict(X_train), y_train, average='macro'))\n",
    "    valid_error.append(f1_score(model.predict(X_valid), y_valid, average='macro'))        \n",
    "\n",
    "print(train_error, np.mean(train_error))\n",
    "print(valid_error, np.mean(valid_error))\n",
    "\n",
    "## Report Test Error\n",
    "model.fit(dev_X_one_hot_std, dev_y)\n",
    "print(f1_score(model.predict(test_X_one_hot_std), test_y, average='macro'))\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0] 1.0\n",
      "[0.28257825339047793, 0.38417923806008114, 0.34245626189928396, 0.3311194025594257] 0.3350832889773172\n",
      "0.44948801285320505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Training your decision tree classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "\n",
    "## Define Decision Tree Model\n",
    "model = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "## Define cross validation using KFold method\n",
    "## cv = ShuffleSplit(n_splits=4, test_size=0.25, random_state=42)\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "## lists that hold error on cross validation\n",
    "train_error = []\n",
    "valid_error = []\n",
    "\n",
    "## Train & Valid Model using Cross Validation\n",
    "for train_index, valid_index in cv.split(dev_X_one_hot_std):\n",
    "    X_train, X_valid = dev_X_one_hot_std.iloc[train_index, :], dev_X_one_hot_std.iloc[valid_index, :]\n",
    "    y_train, y_valid = dev_y.iloc[train_index], dev_y.iloc[valid_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_error.append(f1_score(model.predict(X_train), y_train, average='macro'))\n",
    "    valid_error.append(f1_score(model.predict(X_valid), y_valid, average='macro'))        \n",
    "\n",
    "print(train_error, np.mean(train_error))\n",
    "print(valid_error, np.mean(valid_error))\n",
    "\n",
    "## Report Test Error\n",
    "model.fit(dev_X_one_hot_std, dev_y)\n",
    "print(f1_score(model.predict(test_X_one_hot_std), test_y, average='macro'))\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9135010444630639, 0.9192496171932301, 0.91351310306577, 0.9134420408330031] 0.9149264513887668\n",
      "[0.28632820307592416, 0.35352092290041637, 0.344836616932126, 0.32786500922769735] 0.328137688034041\n",
      "0.4504117349324501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Training your decision tree classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "\n",
    "## Define Decision Tree Model\n",
    "model = DecisionTreeClassifier(class_weight='balanced', max_depth=13, random_state=42)\n",
    "\n",
    "## Define cross validation using KFold method\n",
    "## cv = ShuffleSplit(n_splits=4, test_size=0.25, random_state=42)\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "## lists that hold error on cross validation\n",
    "train_error = []\n",
    "valid_error = []\n",
    "\n",
    "## Train & Valid Model using Cross Validation\n",
    "for train_index, valid_index in cv.split(dev_X_one_hot_std):\n",
    "    X_train, X_valid = dev_X_one_hot_std.iloc[train_index, :], dev_X_one_hot_std.iloc[valid_index, :]\n",
    "    y_train, y_valid = dev_y.iloc[train_index], dev_y.iloc[valid_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_error.append(f1_score(model.predict(X_train), y_train, average='macro'))\n",
    "    valid_error.append(f1_score(model.predict(X_valid), y_valid, average='macro'))        \n",
    "\n",
    "print(train_error, np.mean(train_error))\n",
    "print(valid_error, np.mean(valid_error))\n",
    "\n",
    "## Report Test Error\n",
    "model.fit(dev_X_one_hot_std, dev_y)\n",
    "print(f1_score(model.predict(test_X_one_hot_std), test_y, average='macro'))\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Train and validate your <b>random forest classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0] 1.0\n",
      "[0.3402672430271573, 0.35753343093210904, 0.3926946599250871, 0.328536688789115] 0.3547580056683671\n",
      "0.5171585701310437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Training your random forest classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "\n",
    "## Define Random Forest Model\n",
    "model = RandomForestClassifier(n_estimators=400, random_state=42)\n",
    "\n",
    "## Define cross validation using KFold method\n",
    "## cv = ShuffleSplit(n_splits=4, test_size=0.25, random_state=42)\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "## lists that hold error on cross validation\n",
    "train_error = []\n",
    "valid_error = []\n",
    "\n",
    "## Train & Valid Model using Cross Validation\n",
    "for train_index, valid_index in cv.split(dev_X_one_hot_std):\n",
    "    X_train, X_valid = dev_X_one_hot_std.iloc[train_index, :], dev_X_one_hot_std.iloc[valid_index, :]\n",
    "    y_train, y_valid = dev_y.iloc[train_index], dev_y.iloc[valid_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_error.append(f1_score(model.predict(X_train), y_train, average='macro'))\n",
    "    valid_error.append(f1_score(model.predict(X_valid), y_valid, average='macro'))        \n",
    "\n",
    "print(train_error, np.mean(train_error))\n",
    "print(valid_error, np.mean(valid_error))\n",
    "\n",
    "## Report Test Error\n",
    "model.fit(dev_X_one_hot_std, dev_y)\n",
    "print(f1_score(model.predict(test_X_one_hot_std), test_y, average='macro'))\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "Train and validate your <b>support vector machine classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8399380782786866, 0.8466594757513687, 0.8272389248576366, 0.8000104796348766] 0.8284617396306421\n",
      "[0.37493239303594816, 0.41042156407174796, 0.37905549903420693, 0.31854284363790797] 0.3707380749449528\n",
      "0.40380827505740763\n"
     ]
    }
   ],
   "source": [
    "# Training your support vector machine classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "\n",
    "## Define SVC Model\n",
    "model = SVC(C=100, gamma=0.01, random_state=42)\n",
    "\n",
    "## Define cross validation using KFold method\n",
    "## cv = ShuffleSplit(n_splits=4, test_size=0.25, random_state=42)\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "## lists that hold error on cross validation\n",
    "train_error = []\n",
    "valid_error = []\n",
    "\n",
    "## Train & Valid Model using Cross Validation\n",
    "for train_index, valid_index in cv.split(dev_X_one_hot_std):\n",
    "    X_train, X_valid = dev_X_one_hot_std.iloc[train_index, :], dev_X_one_hot_std.iloc[valid_index, :]\n",
    "    y_train, y_valid = dev_y.iloc[train_index], dev_y.iloc[valid_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_error.append(f1_score(model.predict(X_train), y_train, average='macro'))\n",
    "    valid_error.append(f1_score(model.predict(X_valid), y_valid, average='macro'))        \n",
    "\n",
    "print(train_error, np.mean(train_error))\n",
    "print(valid_error, np.mean(valid_error))\n",
    "\n",
    "## Report Test Error\n",
    "model.fit(dev_X_one_hot_std, dev_y)\n",
    "print(f1_score(model.predict(test_X_one_hot_std), test_y, average='macro'))\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Option) Other Classifiers.\n",
    "Train and validate other classifiers by your own manner.\n",
    "> <b> If you need, you can import other models only in this cell, only in scikit-learn. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9974053165703362, 0.9961638167623205, 0.9953008824496936, 0.9963410149747092] 0.9963027576892648\n",
      "[0.39766907193869305, 0.3600243976713391, 0.36054648910955794, 0.4019046311187333] 0.38003614745958086\n",
      "0.5700837403813704\n"
     ]
    }
   ],
   "source": [
    "# If you need additional packages, import your own packages below.\n",
    "# Your Code Here\n",
    "\n",
    "## Define Random Forest Model\n",
    "model = GradientBoostingClassifier(n_estimators=400, random_state=42)\n",
    "\n",
    "## Define cross validation using KFold method\n",
    "## cv = ShuffleSplit(n_splits=4, test_size=0.25, random_state=42)\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "## lists that hold error on cross validation\n",
    "train_error = []\n",
    "valid_error = []\n",
    "\n",
    "## Train & Valid Model using Cross Validation\n",
    "for train_index, valid_index in cv.split(dev_X_one_hot_std):\n",
    "    X_train, X_valid = dev_X_one_hot_std.iloc[train_index, :], dev_X_one_hot_std.iloc[valid_index, :]\n",
    "    y_train, y_valid = dev_y.iloc[train_index], dev_y.iloc[valid_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_error.append(f1_score(model.predict(X_train), y_train, average='macro'))\n",
    "    valid_error.append(f1_score(model.predict(X_valid), y_valid, average='macro'))        \n",
    "\n",
    "print(train_error, np.mean(train_error))\n",
    "print(valid_error, np.mean(valid_error))\n",
    "\n",
    "## Report Test Error\n",
    "model.fit(dev_X_one_hot_std, dev_y)\n",
    "print(f1_score(model.predict(test_X_one_hot_std), test_y, average='macro'))\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Jinhe\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9662254596051988, 0.9706982107192488, 0.9568456040795452, 0.9682956215286964] 0.9655162239831723\n",
      "[0.36791659442360597, 0.41323924106208004, 0.34898957127269026, 0.4315990317773184] 0.39043610963392367\n",
      "(1600, 23) (1600,)\n",
      "0.5780655086039201\n"
     ]
    }
   ],
   "source": [
    "# If you need additional packages, import your own packages below.\n",
    "# Your Code Here\n",
    "\n",
    "## Define Random Forest Model\n",
    "model = GradientBoostingClassifier(learning_rate=0.18, n_estimators=100, random_state=42)\n",
    "\n",
    "## Define cross validation using KFold method\n",
    "## cv = ShuffleSplit(n_splits=4, test_size=0.25, random_state=42)\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "## lists that hold error on cross validation\n",
    "train_error = []\n",
    "valid_error = []\n",
    "\n",
    "## Train & Valid Model using Cross Validation\n",
    "for train_index, valid_index in cv.split(dev_X_one_hot_std):\n",
    "    X_train, X_valid = dev_X_one_hot_std.iloc[train_index, :], dev_X_one_hot_std.iloc[valid_index, :]\n",
    "    y_train, y_valid = dev_y.iloc[train_index], dev_y.iloc[valid_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_error.append(f1_score(model.predict(X_train), y_train, average='macro'))\n",
    "    valid_error.append(f1_score(model.predict(X_valid), y_valid, average='macro'))        \n",
    "\n",
    "print(train_error, np.mean(train_error))\n",
    "print(valid_error, np.mean(valid_error))\n",
    "\n",
    "## Report Test Error\n",
    "model.fit(dev_X_one_hot_std, dev_y)\n",
    "print(f1_score(model.predict(test_X_one_hot_std), test_y, average='macro'))\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit your prediction on the test data.\n",
    "\n",
    "* Select your model and explain it briefly.\n",
    "* You should read <b>\"test.csv\"</b>.\n",
    "* Prerdict your model in array form.\n",
    "* Prediction example <br>\n",
    "[2, 6, 14, 8, $\\cdots$]\n",
    "* We will rank your result by <b>F1 metric(with 'macro' option)</b>.\n",
    "* <b> If you don't submit prediction file or submit it in wrong format, you can't get the point for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Explain your final model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define linear regression function\n",
    "# You may use sklearn.linear_model.LinearRegression\n",
    "# Your code here\n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "# End your code\n",
    "\n",
    "# Basic settings. DO NOT MODIFY\n",
    "selected_feature = []\n",
    "sel_num = 23\n",
    "valid_split = 1/10\n",
    "cv = ShuffleSplit(n_splits=10, test_size=valid_split, random_state=0)\n",
    "\n",
    "selected_train_error = []\n",
    "selected_valid_error = []\n",
    "\n",
    "# For greedy selection\n",
    "for sel in range(sel_num) :\n",
    "    min_train_error = 0\n",
    "    min_valid_error = 0\n",
    "    min_feature = 0\n",
    "    \n",
    "    # For each feature\n",
    "    for i in X_dev.columns:\n",
    "        train_error_ith = []\n",
    "        valid_error_ith = []\n",
    "        # Select feature greedy\n",
    "        # Hint : There should be no duplicated feature in selected_feature\n",
    "        # Your code here\n",
    "        if (i in selected_feature): continue\n",
    "        else: X_dev_fs = X_dev[selected_feature + [i]]\n",
    "        # End your code\n",
    "        \n",
    "        # For cross validation\n",
    "        for train_index, test_index in cv.split(X_dev) :\n",
    "            X_train, X_valid = X_dev_fs.iloc[train_index, :], X_dev_fs.iloc[test_index, :]\n",
    "            y_train, y_valid = y_dev.iloc[train_index], y_dev.iloc[test_index]\n",
    "        \n",
    "            # Derive training error, validation error\n",
    "            # You may use sklearn.metrics.mean_squared_error, model.fit(), model.predict()\n",
    "            # Your code here\n",
    "            model.fit(X_train, y_train)\n",
    "            train_error_ith.append(f1_score(model.predict(X_train), y_train, average='macro'))\n",
    "            valid_error_ith.append(f1_score(model.predict(X_valid), y_valid, average='macro'))\n",
    "            # End your code\n",
    "            \n",
    "        # Select best performance feature set on each features\n",
    "        # You should choose the feature which has minimum mean cross validation error\n",
    "        # Your code here\n",
    "        if (np.mean(valid_error_ith) > min_valid_error):\n",
    "            min_feature = i\n",
    "            min_train_error = np.mean(train_error_ith)\n",
    "            min_valid_error = np.mean(valid_error_ith)\n",
    "            print(valid_error_ith)\n",
    "        # End your code\n",
    "    \n",
    "    print('='*50)\n",
    "    print(\"# of selected feature(s) : {}\".format(sel+1))\n",
    "    print(\"Selected feature of this iteration : {}\".format(min_feature))\n",
    "    selected_feature.append(min_feature)\n",
    "    selected_train_error.append(min_train_error)\n",
    "    selected_valid_error.append(min_valid_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train error plot\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(np.arange(1,sel_num+1), selected_train_error)\n",
    "plt.title('Training error')\n",
    "plt.legend(['error'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation error plot\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(np.arange(1,sel_num+1), selected_valid_error)\n",
    "plt.title('Validation error')\n",
    "plt.legend(['error'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select optimal feature set corresponding the minimum cross validation error\n",
    "# Your code here\n",
    "min_valid_error = 0\n",
    "min_validation_index = 0\n",
    "\n",
    "for i in range(0, sel_num):\n",
    "    if (selected_valid_error[i] > min_valid_error):\n",
    "        min_valid_error = selected_valid_error[i]\n",
    "        min_validation_index = i\n",
    "        \n",
    "X_dev_fs = X_dev[selected_feature[:min_validation_index + 1]]\n",
    "selected_feature = selected_feature[:min_validation_index + 1]\n",
    "# End your code\n",
    "\n",
    "# Basic settings. DO NOT MODIFY\n",
    "min_train_error = 0\n",
    "min_valid_error = 0\n",
    "optimal_param = np.array([])\n",
    "optimal_model = 0\n",
    "\n",
    "for train_index, test_index in cv.split(X_dev) :\n",
    "    \n",
    "    X_train, X_valid = X_dev_fs.iloc[train_index, :], X_dev_fs.iloc[test_index, :]\n",
    "    y_train, y_valid = y_dev.iloc[train_index], y_dev.iloc[test_index]\n",
    "    \n",
    "    # Derive training error, validation error for each fold\n",
    "    # For each fold, you need to compare error with previous minimum error.\n",
    "    # Your code here\n",
    "    model.fit(X_train, y_train)\n",
    "    if (f1_score(model.predict(X_train), y_train, average='macro') > min_valid_error):\n",
    "        min_train_error = f1_score(model.predict(X_train), y_train, average='macro')\n",
    "        min_valid_error = f1_score(model.predict(X_valid), y_valid, average='macro')\n",
    "        \n",
    "        optimal_param = model.get_params()\n",
    "        optimal_model = model\n",
    "    # End your code\n",
    "\n",
    "# Find the best model on each fold\n",
    "# Derive test error with best performance model\n",
    "# Your code here\n",
    "X_test_drop = X_test[selected_feature[:min_validation_index + 1]]\n",
    "model.set_params(**optimal_param)\n",
    "test_error = f1_score(optimal_model.predict(X_test_drop), y_test, average='macro')\n",
    "# End your code\n",
    "\n",
    "# Drop features of final model\n",
    "print(\"Results\")\n",
    "print(\"# of selected features : {}\".format(len(selected_feature)))\n",
    "print(\"Selected features : \")\n",
    "print(selected_feature)\n",
    "\n",
    "# Drop test error and accuracy\n",
    "print(\"Training error : {}\".format(min_train_error))\n",
    "print(\"Validation error : {}\".format(min_valid_error))\n",
    "print(\"Test error : {}\".format(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load test dataset.\n",
    "# Your Code Here\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target class\n",
    "# Make variable \"my_answer\", type of array, and fill this array with your class predictions.\n",
    "# Modify file name into your student number and your name.\n",
    "# Your Code Here\n",
    "\n",
    "\n",
    "file_name = \"HW2_2016320198_.csv\"\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This section is for saving predicted answers. DO NOT MODIFY.\n",
    "pd.Series(my_answer).to_csv(\"./data/\" + file_name, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
